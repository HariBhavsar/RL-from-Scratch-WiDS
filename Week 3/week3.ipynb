{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 3\n",
    "## Efficiently finding optimal policies in MABs\n",
    "\n",
    "In this assignment, we will work with Multi Armed Bandit environments, and try to find the best policies using different strategies to minimize the total regret.\n",
    "\n",
    "The aim of this exercise is to code agents capable of understanding the underlying probability distributions of the environment and finding the most optimal actions as early as possible.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary stuff\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# if you want to use envs from Gym, import it\n",
    "# import gym, gym_bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple **2-armed Bernoulli** bandit.\n",
    "\n",
    "If you want a cleaner code, you can implement Bandits using `class` in Python.\n",
    "\n",
    "We have included sample code for this in `bandits.py` which you can take/import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the underlying probability distribution\n",
    "\n",
    "probs = np.random.random(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our MDP is a function which takes an action and returns a reward\n",
    "\n",
    "def mab_2_env(action):\n",
    "    gen = np.random.random()\n",
    "\n",
    "    # for bernoulli bandits, the reward is 1 if the random number is less than the probability of success, else 0\n",
    "    return gen < probs[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a template for testing different strategies.\n",
    "\n",
    "Feel free to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strategy(env, n_actions, selector, n_episodes = 1000):\n",
    "    \n",
    "    # initialize Q and N to 0s\n",
    "    Q = np.zeros(n_actions)\n",
    "    N = np.zeros(n_actions)\n",
    "    rewards = 0\n",
    "    actions = []\n",
    "    # loop for n_episodes\n",
    "    for e in range(0,n_episodes):\n",
    "        \n",
    "        # selector function takes in current Q and returns an action\n",
    "        # modify the selector function according to the strategy\n",
    "        if selector == exponentially_decaying_epsilon_greedy:\n",
    "            action = selector(Q,0.5,e)\n",
    "        elif selector == epsilon_greedy:\n",
    "            action = selector(Q,0.1)\n",
    "        elif selector == random_selector:\n",
    "            action = selector(Q)\n",
    "\n",
    "        elif selector == softmax_strategy:\n",
    "            action = selector(Q,0.9,e)\n",
    "        elif selector == ucb:\n",
    "            action = selector(Q,N,1,e)\n",
    "        elif selector == thompson_sampling:\n",
    "            action = selector(Q,N,0.5,0.5)\n",
    "\n",
    "\n",
    "        # get the reward from the environment\n",
    "        reward = env(action)\n",
    "        rewards += reward\n",
    "        actions.append(action)\n",
    "        # update N and Q\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action])/N[action]\n",
    "\n",
    "    # return the best action\n",
    "    return np.argmax(Q),rewards,actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the simplest selector using pure-exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector returns a random action\n",
    "random_selector = lambda Q : np.random.randint(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "test_strategy(mab_2_env, 2, random_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns the optimal action. Let's check if that's indeed true.\n",
    "\n",
    "We can do that by revealing the actual `probs` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80499935, 0.65065254])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our pure exploration strategy does indeed return the optimal action for this Bernoulli bandit. \n",
    "\n",
    "You can try generating new bandits with different `probs` and try out the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this in place, here's what you have to do -\n",
    "\n",
    "Recall that, the regret $\\mathcal{T}$ is given by,\n",
    "\n",
    "$$\\mathcal{T}=\\sum  _{e=1} ^{E} \\mathbb{E} \\left[ v_* - q_* \\left( A_e \\right) \\right]$$\n",
    "\n",
    "We can only calculate it when we have the $v_*$ and $q_*$ functions known beforehand. Since we are making the MDPs from scratch, that's not an issue for us right now.\n",
    "\n",
    "But remember, in real-life problems, these functions are not known. Hence we must be aware of multiple policy finding strategies and try the one which gives best results fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 0\n",
    "\n",
    "Implement the calculation of the total regret $\\mathcal{T}$ for your strategy.\n",
    "\n",
    "To do this, you will need to store the rewards obtained each episode. Modify the `strategy` function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total regret\n",
    "\n",
    "def regret(rewards, probs):\n",
    "\n",
    "    # implement the regret function here\n",
    "    #print(rewards)\n",
    "    size = len(rewards)\n",
    "    u = 0\n",
    "    for i in range(0,len(probs)):\n",
    "        if probs[i] > probs[u]:\n",
    "            u = i\n",
    "    t1 = probs[u]*size\n",
    "    sum = 0\n",
    "    for i in range(0,size):\n",
    "        sum += probs[rewards[i]]\n",
    "    #print('t1 is ',t1)\n",
    "    #print('sum is ',sum)\n",
    "    return t1-sum\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 1\n",
    "\n",
    "Now, let's implement some other selection strategies and compare their regret with the simple exploration strategy.\n",
    "\n",
    "Note that some of these strategies involve hyperparameter(s) which need to be manually adjusted. You have to play around with the values and see which one gives you best results.\n",
    "\n",
    "This is known as \"hyperparameter tuning\" and is quite commonly done while working with complex models (including neural networks). Personally, you should try out some natural values (including the ones given in the book) along with some extreme values where it is easy to manually verify the correctness of your strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon-greedy strategy\n",
    "# Already implemented for you coz I am nice\n",
    "\n",
    "def epsilon_greedy(Q, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponentially decaying epsilon greedy strategy\n",
    "\n",
    "def exponentially_decaying_epsilon_greedy(Q, epsilon, episode):\n",
    "    \n",
    "    # implement the exponentially decaying epsilon greedy strategy here\n",
    "\n",
    "    nums = epsilon**(episode - 1)\n",
    "    if np.random.random() < nums:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)\n",
    "\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax action selection strategy\n",
    "\n",
    "def softmax_strategy(Q, temperature,episode):\n",
    "    \n",
    "    # implement the softmax strategy here\n",
    "    if episode == 1:\n",
    "        return np.random.randint(len(Q))\n",
    "    nums = temperature**(episode - 1)\n",
    "    if nums < 0.001:\n",
    "        nums = 0.001\n",
    "    scaledQ = Q/nums\n",
    "    normQ = scaledQ - np.max(scaledQ)\n",
    "    expQ = np.exp(normQ)\n",
    "    prob = expQ/np.sum(expQ)\n",
    "    ac = np.random.choice(np.arange(len(prob)),size=1,p=prob)[0]\n",
    "    return ac\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper confidence bound strategy\n",
    "\n",
    "def ucb(Q, N, c, episode):\n",
    "    \n",
    "    # implement the ucb strategy here\n",
    "    for i in range(0,len(N)):\n",
    "        if N[i] == 0:\n",
    "            return i\n",
    "    arr = 1/N\n",
    "    arr = arr*np.log(episode)\n",
    "    arr = np.sqrt(arr)\n",
    "    arr = arr*c\n",
    "    arr = arr + Q\n",
    "    return np.argmax(arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thompson sampling strategy\n",
    "\n",
    "def thompson_sampling(Q, N, alpha, beta):\n",
    "    \n",
    "    # implement the thompson sampling strategy here\n",
    "    for i in range(0,len(N)):\n",
    "        if N[i] == 0:\n",
    "            return i\n",
    "    samples = np.random.normal(loc=Q, scale=alpha/(np.sqrt(N) + beta))\n",
    "    ac = np.argmax(samples)\n",
    "    return ac\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 2\n",
    "\n",
    "Run each strategy for 2-armed bandit environment and compare the total regrets.\n",
    "\n",
    "You can also try plotting the regret vs episode graph and check if it matches the expected result from Grokking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 3\n",
    "\n",
    "The 2-armed bandits might be too simple for us to actually see substantial difference in the regret of these strategies. \n",
    "\n",
    "Let's now create a more complicated bandit environment and replicate our results on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a 10-armed Gaussian bandit. \n",
    "\n",
    "As required, it will have possible actions and each action will generate a reward sampled from a Gaussian distribution.\n",
    "\n",
    "Hence, each \"arm\" will have a randomly generated $\\mu$ and $\\sigma$, and the rewards will be generated with probabilities following the $\\mathcal{N}(\\mu, \\sigma^2)$ distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 arm gaussian bandit\n",
    "# generate the means for each arm\n",
    "means = np.random.rand(10)\n",
    "\n",
    "# generate the variance for each arm\n",
    "variances = np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our MDP is a again function which takes an action and returns a reward\n",
    "\n",
    "def mab_10_env(action):\n",
    "\n",
    "    # for gaussian bandits, the reward is generated from a normal distribution\n",
    "    return np.random.normal(means[action],np.sqrt(variances[action]))\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 4\n",
    "\n",
    "Test the different strategies on the 10-armed gaussian bandit and verify your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
